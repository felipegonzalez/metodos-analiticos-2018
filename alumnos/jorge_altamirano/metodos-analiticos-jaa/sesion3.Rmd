---
title: 'Sesión 3: 2018/01/29'
output:
  html_document:
    df_print: paged
---

_175904 - Jorge III Altamirano Astorga_


```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(textreuse)
library(stringr)
```

```{r}
limpiar <- function(lineas,...){
  df_lista <- str_split(lineas, ' ') %>% 
    keep(function(x) x[1] != '#') %>%
    transpose %>%
    map(function(col) as.character(col)) 
  df <- data_frame(articulo = df_lista[[1]], 
                   categorias = df_lista[[2]]) 
  df
}
filtrado <- read_lines_chunked('../../../datos/similitud/wiki-100000.txt',
                    skip = 1, callback = ListCallback$new(limpiar))
articulos_df <- filtrado %>% bind_rows %>%
                group_by(articulo) %>%
                summarise(categorias = list(categorias))
set.seed(99)
muestra <- articulos_df %>% sample_n(10)
muestra
```

```{r}
muestra$categorias[[10]]
```

```{r}
cb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
graficar_curvas <- function(df_br, colour = TRUE){
  r <- df_br$r
  b <- df_br$b
  datos_graf <- data_frame(s = seq(0, 1, 0.01))
  curvas_similitud <- data_frame(b = b, r =r) %>%
                    group_by(r, b) %>%
                    mutate(datos = map2(r, b, function(r, b){
                      datos_graf %>% 
                      mutate(prob = 1 - (1 - s ^ r) ^b)
                    })) %>%
                    unnest
  graf_salida <- ggplot(curvas_similitud, 
                        aes(x = s, y = prob, 
                            colour = as.factor(interaction(b,r)))) +
                 geom_line(size=1.1) + 
                 labs(x = 'similitud', y= 'probablidad de ser candidato',
                      colour = 'b.r') 
  if(colour){
    graf_salida + scale_colour_manual(values=cb_palette)
  }
                 
  graf_salida
}
lsh_half <- function(h, b){
   (1 - (0.5) ^ ( 1/b))^(b/h)
}
b <- 20
num_hashes <- 60
lsh_half(num_hashes, b = b)
## [1] 0.3241633
graficar_curvas(data_frame(b = b, r = num_hashes/b)) +
                 geom_vline(xintercept = .4) 
```

```{r}
options("mc.cores" = 14L)
# esta es la función que vamos a usar:
tokenize_sp <- function(x) str_split(x, ' ', simplify = TRUE)
# aunque otra opción es:
minhashes <- minhash_generator(num_hashes, seed = 1223)
# esta línea solo es necesaria porque TextReuseCorpus espera una
# línea de texto, no un vector de tokens.
textos <- articulos_df$categorias %>% 
          lapply(function(x) paste(x, collapse = ' ')) %>%
          as.character
names(textos) <- articulos_df$articulo
system.time(
wiki_corpus <-  TextReuseCorpus(
                text = textos, 
                tokenizer = tokenize_sp,
                minhash_func = minhashes,
                skip_short = FALSE)
)
```

```{r}
str(wiki_corpus[[1002]])
```

```{r}
lsh_wiki <- lsh(wiki_corpus, bands = 20)
lsh_wiki %>% sample_n(20)

```

